{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://paperswithcode.com/task/abstractive-text-summarization"
      ],
      "metadata": {
        "id": "KBXKz6BRWVFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstractive summarization with BART model fine tuned on CNN daily mail \n",
        "\n"
      ],
      "metadata": {
        "id": "mH9rAgUQRO7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERx85lZTSFWH",
        "outputId": "5c2569f4-6d18-42ca-aa7b-06ac0dab8217"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "IDZdCxkRRAe_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoTokenizer\n",
        "# Import stopwords module\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Import the regular expression module\n",
        "import re\n",
        "\n",
        "import pprint as pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9yncde6dGoL",
        "outputId": "c5fb40e3-3580-4b2e-c84f-fd0fa8ce88e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file in read mode\n",
        "f = open(\"message.txt\", \"r\")\n",
        "# Read the file content into a string variable\n",
        "short_aws_resume = f.read()\n",
        "# Close the file\n",
        "f.close()\n",
        "#remove non breaking space \n",
        "short_aws_resume = short_aws_resume.replace(\"\\xa0\", \" \")\n",
        "#remove not usefull white space \n",
        "short_aws_resume = re.sub(\"\\s+\", \" \", short_aws_resume)\n",
        "# Print the variable\n",
        "print(short_aws_resume)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRuiqyTbUnA-",
        "outputId": "28ba0aa0-1e92-4a2b-98b5-c10bb8b9c974"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de la pour merci alors alors je vous l'ai dit mais je le répète le cours à distance ça nécessite encore plus d'interactivité de votre part pour que pour qu'il se déroule dans les meilleures conditions et pour que vous vous en profitez au maximum d'accord donc je compte sur vous pour poser des questions il n'y a jamais de question bête tout est intéressant toute toute interaction et il ne ça va que aider le cours je ferme ce bazar derrière moi oui du coup donc donc aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information pourquoi dans notre architecture on peut avoir besoin d'une solution d'archivage alors pour la solution d'archivage ça serait quand on a des données auquel on accède pas tous les jours et donc on gagne de l'espace ou des trimmons des performances d'accès oui voilà on réduit on cherche à avoir une solution de stockage qui est à faible coût tout en conservant le données au cas où et et tout en aussi en faisant des économies on va pas trop dépenser juste pour conserver des données que potentiellement on va jamais son serveur mais on est contraint de les garder au cas où du coup les sides je vous les présentez je pense pas si on est arrivé à quel stade pour les sides juste le 38 ça permet de rappeler le contexte je le fais rapidement donc l'idée c'est ça donc voilà donc c'est un service de stockage de données c'est encore une fois c'est comme les bases données avec la différence c'est que les bases données souvent ça coûte plus cher et ils sont destinés pour d'avoir des données qui soit mis à jour qui soit qui soit consultable souvent très fréquemment voilà et ça coûte plus cher que les solutions d'archivage donc le le s3 ça permet de garder une bonne disponibilité par rapport ça on le dit par rapport à d'autres solutions d'archivage évidemment c'est pas c'est pas les mêmes pertes des bases données donc voilà avec la pouf avec une scalabilité flexible avec la sécurité avec les performances donc au niveau des des caractéristiques les caractéristiques donc avec s3 on est capable de stocker des fichiers jusqu'à la taille de 5 terres le stockage il est illimité les stockage il est fait dans des baguettes les baguettes c'est les baguettes c'est juste une organisation logique de nos données c'est comme les répertoires sur la machine voilà le name space c'est universel je vous rappelle que je vous ai dit cette cette information c'est à dire quoi c'est à dire si moi je choisis un nom de baguette enfin c'est unique pas seulement dans mon name space par rapport à mon user assign non c'est pas par rapport à mon alias seulement mais c'est unique à l'échelle mondiale à l'échelle AWS voilà à l'échelle universelle c'est à dire quoi si moi je faisais un nom aucun parmi vous qui peut le choisir voilà chacun de vous lorsqu'après dans le tp il va tenter de faire la création d'un baguette il va se rendre compte qu'il ne peut le faire qu'il ne peut le faire qu'à condition qu'il peut il ne peut créer des baguettes qu'avec des noms qui n'ont jamais été créés en par quelqu'un d'autre mais c'est quoi d'utilité de faire ça c'est un choix c'est un choix de conception de la solution ils ont dit c'est universel ils ont dit c'est universel j'ai pas enfin en fait si nous par exemple on souhaite réserver un name space propre à nous bah c'est simple on fait un baguette qu'on l'appelle par exemple osgi yabd3 et et et et et on met tous nos données sous ce baguette et du coup donc on a le baguette réservée à nous personne d'autre ne peut l'utiliser et et voilà ça permet c'est une façon que je trouve flexible pour choisir les noms des des baguettes et pour que pour se réserver aussi des noms des baguettes alors au niveau des des gaanties donc il offre 99,9% de disponibilité du données donc voilà c'est à dire quoi c'est à dire sur une année on risque d'avoir un sur fin sur mille jours on risque d'avoir un jour de un jour d'indisponibilité au total au total sur mille jours donc c'est à dire sur trois ans on n'a pas un risque d'avoir un jour d'indisponibilité et pour la durabilité c'est c'est encore voilà très précis c'est à dire quoi c'est à dire le risque de perdre une donnée ou d'avoir un problème sur une donnée fin ça représente un fois 10 puissance au moins moins 12 ou moins 13 même pour son risque donc c'est très très très très limité comme risque alors la la maintenant la notion d'objet l'objet ou fichier en fait on dit objet ou bien fichier et là non et là un contenu sont le contenu c'est la data c'est la séquence de bit et tout la version il a une version il peut avoir une version si on choisit d'activer le versioning et il peut avoir d'autres méthodes à la souplementaire donc il peut avoir aussi des ressources particulièrement les acl les acl ce sont des droits en fait ce sont des c'est une définition des droits sur les ressources alors les fonctionnalités alors je reviens à la remarque que j'ai dit c'est que les solutions d'archivage ils sont faites pour les données qui sont pas fréquemment consultables qui sont pas fréquemment consultables et et encore plus et encore moins fréquemment mis à jour c'est ça en fait les solutions d'archivage et et l'avantage avec elle soit c'est que cette disponibilité n'est pas binaire en fait n'est pas binaire c'est pas c'est pas soit des données qui sont qui sont hautement disponibles sur des bases de données soit ils sont ils sont archivés du coup ils sont beaucoup moins disponibles voilà ils peuvent être chargés après une heure et on peut les requêter une ou deux fois par jour non c'est pas aussi c'est pas du tout binaire mais plutôt il y a une disponibilité archique dans le cours dans le chapitre suivant on va parler juste après de de des classes des classes de disponibilité de classe de au niveau de s3 et ce qui permet d'avoir des disponibilités différentes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "7vFXP_wnRFQQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the length of each part\n",
        "part_length = len(short_aws_resume) // 3"
      ],
      "metadata": {
        "id": "4xiEoMBKVqAZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first part by slicing from the start to part_length\n",
        "first_part = short_aws_resume[:part_length]\n",
        "# Get the second part by slicing from part_length to part_length * 2\n",
        "second_part = short_aws_resume[part_length:part_length * 2]\n",
        "# Get the third part by slicing from part_length * 2 to the end\n",
        "third_part = short_aws_resume[part_length * 2:]"
      ],
      "metadata": {
        "id": "iSlE8wqMWI62"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(first_part, max_length=130, min_length=30, do_sample=True, num_return_sequences=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jke8JVIrRJjn",
        "outputId": "c75ded5e-a9c2-48f2-8cf9-e7ec9efc4a36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'Aujourd\\'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d\\'information. \"On peut avoir besoin d\\'une solution d\\'archivage alors pour la solution d\\', on serait quand on a des données auquel on accède pas tous les jours et donc on gagne de l\\'espace ou des trimmons des performances d\\'accès\"'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(second_part, max_length=130, min_length=30, do_sample=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LAPzJcaW0W6",
        "outputId": "6dce0570-00d4-48bb-e5cc-821eb51d24b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"Avec s3 on est capable de stocker des fichiers jusqu'à la taille de 5 terres. Le stockage il est illimité les baguettes. On a le baguette réservée à nous personne d'autre ne peut l'utiliser.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(third_part, max_length=130, min_length=30, do_sample=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C9yni5_W0uy",
        "outputId": "de1cc9c0-4852-439d-d11a-7fb62c13e3f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"On n'a pas un risque d'avoir un jour d'indisponibilité. C'est c'est encore voilà très précis. On peut avoir des ressources particulièrement les acl.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stop words to compare quality of summaries without them"
      ],
      "metadata": {
        "id": "1-2jZ6HjcgRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize your string into words\n",
        "words = word_tokenize(first_part)\n",
        "\n",
        "# Define the stop words for both languages\n",
        "stop_words = stopwords.words(\"english\") + stopwords.words(\"french\")\n",
        "\n",
        "# Remove the stop words from your words\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Print the filtered words\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMpJ8TdBcfA4",
        "outputId": "3e54af96-2767-4db4-f644-8913d085bd23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['merci', 'alors', 'alors', \"l'ai\", 'dit', 'répète', 'cours', 'distance', 'ça', 'nécessite', 'encore', 'plus', \"d'interactivité\", 'part', \"qu'il\", 'déroule', 'meilleures', 'conditions', 'profitez', 'maximum', \"d'accord\", 'donc', 'compte', 'poser', 'questions', \"'\", 'jamais', 'question', 'bête', 'tout', 'intéressant', 'toute', 'toute', 'interaction', 'ça', 'va', 'aider', 'cours', 'ferme', 'bazar', 'derrière', 'oui', 'coup', 'donc', 'donc', \"aujourd'hui\", 'va', 'passer', 'chapitre', 'suivant', 'alors', 'si', 'pose', 'question', 'pourquoi', 'système', \"d'information\", 'pourquoi', 'architecture', 'peut', 'avoir', 'besoin', \"d'une\", 'solution', \"d'archivage\", 'alors', 'solution', \"d'archivage\", 'ça', 'quand', 'données', 'auquel', 'accède', 'tous', 'jours', 'donc', 'gagne', \"l'espace\", 'trimmons', 'performances', \"d'accès\", 'oui', 'voilà', 'réduit', 'cherche', 'avoir', 'solution', 'stockage', 'faible', 'coût', 'tout', 'conservant', 'données', 'cas', 'où', 'tout', 'aussi', 'faisant', 'économies', 'va', 'trop', 'dépenser', 'juste', 'conserver', 'données', 'potentiellement', 'va', 'jamais', 'serveur', 'contraint', 'garder', 'cas', 'où', 'coup', 'sides', 'présentez', 'pense', 'si', 'arrivé', 'quel', 'stade', 'sides', 'juste', '38', 'ça', 'permet', 'rappeler', 'contexte', 'fais', 'rapidement', 'donc', \"l'idée\", \"c'est\", 'ça', 'donc', 'voilà', 'donc', \"c'est\", 'service', 'stockage', 'données', \"c'est\", 'encore', 'fois', \"c'est\", 'comme', 'bases', 'données', 'différence', \"c'est\", 'bases', 'données', 'souvent', 'ça', 'coûte', 'plus', 'cher', 'destinés', \"d'avoir\", 'données', 'mis', 'jour', 'consultable', 'souvent', 'très', 'fréquemment', 'voilà', 'ça', 'coûte', 'plus', 'cher', 'solutions', \"d'archivage\", 'donc', 's3', 'ça', 'permet', 'garder', 'bonne', 'disponibilité', 'rapport', 'ça', 'dit', 'rapport', \"d'autres\", 'solutions', \"d'archivage\", 'évidemment', \"c'est\", \"c'est\", 'mêmes', 'pertes', 'bases', 'données', 'donc', 'voilà', 'pouf', 'scalabilité']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ1g2SAugpKi",
        "outputId": "b45db7fe-ee3b-4b55-b68c-38aaf3f963d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_part_without_stopwords = \" \".join(filtered_words)"
      ],
      "metadata": {
        "id": "OlTiK8DbdmWG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(first_part, max_length=512, min_length=56, do_sample=False, num_beams=5, num_return_sequences=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b26G8nkleOE9",
        "outputId": "e7ef5cd9-43ba-4db8-cdad-13dab10f980d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"Aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information. Le le s3 permet de garder une bonne disponibilité par rapport.\"}, {'summary_text': \"Aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information. Le s3 permet de garder une bonne disponibilité par rapport.\"}, {'summary_text': \"Aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information. On peut avoir besoin d'une solution d'archivage alors pour la solution. de la pour merci.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_part)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-QxiyhQfpRp",
        "outputId": "2a71a98d-c72a-4ce6-e43c-4b2282920121"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de la pour merci alors alors je vous l'ai dit mais je le répète le cours à distance ça nécessite encore plus d'interactivité de votre part pour que pour qu'il se déroule dans les meilleures conditions et pour que vous vous en profitez au maximum d'accord donc je compte sur vous pour poser des questions il n'y a jamais de question bête tout est intéressant toute toute interaction et il ne ça va que aider le cours je ferme ce bazar derrière moi oui du coup donc donc aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information pourquoi dans notre architecture on peut avoir besoin d'une solution d'archivage alors pour la solution d'archivage ça serait quand on a des données auquel on accède pas tous les jours et donc on gagne de l'espace ou des trimmons des performances d'accès oui voilà on réduit on cherche à avoir une solution de stockage qui est à faible coût tout en conservant le données au cas où et et tout en aussi en faisant des économies on va pas trop dépenser juste pour conserver des données que potentiellement on va jamais son serveur mais on est contraint de les garder au cas où du coup les sides je vous les présentez je pense pas si on est arrivé à quel stade pour les sides juste le 38 ça permet de rappeler le contexte je le fais rapidement donc l'idée c'est ça donc voilà donc c'est un service de stockage de données c'est encore une fois c'est comme les bases données avec la différence c'est que les bases données souvent ça coûte plus cher et ils sont destinés pour d'avoir des données qui soit mis à jour qui soit qui soit consultable souvent très fréquemment voilà et ça coûte plus cher que les solutions d'archivage donc le le s3 ça permet de garder une bonne disponibilité par rapport ça on le dit par rapport à d'autres solutions d'archivage évidemment c'est pas c'est pas les mêmes pertes des bases données donc voilà avec la pouf avec une scalabilité\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(first_part_without_stopwords, max_length=512, min_length=56, do_sample=False, num_beams=5, num_return_sequences=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUIPeR3ofJXk",
        "outputId": "8f0ebea7-8911-40aa-ef83-da55011e1ea5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 512, but you input_length is only 478. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=239)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"L'idée c'est ça donc voilà donc c'eux-même. l'archivage données coûte plus cher destinés d'avoirs mis jour consultable souvent très fréquemment voilès. L'architecture peut avoir besoin d'une solution d' archivage faible coût tout aussi faisant économies.\"}, {'summary_text': \"L'idée c'est ça donc voilà donc c'eux-même. l'archivage données coûte plus cher destinés d'avoirs mis jour consultable souvent très fréquemment voilès. L'architecture peut avoir besoin d'une solution d' archivage faible coût tout conservant don'tées.\"}, {'summary_text': \"L'idée c'est ça donc voilà donc c'eux-même. l'archivage données coûte plus cher destinés d'avoirs mis jour consultable souvent très fréquemment voilès. L'architecture peut avoir besoin d'une solution d' archivage faible coût tout conservant don'tées cas.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer(first_part_without_stopwords, max_length=478, min_length=160, do_sample=False, num_beams=7, num_return_sequences=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HUqwZSbf0w1",
        "outputId": "3cfc2bc7-0320-4ece-99fa-d5c6bc048240"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"L'idée c'est ça donc voilà donc c'évidemment. Service stockage données coûte plus cher solutions d'archivage donc s3. Aujourd'hui va passer chapitre suivant alors si pose question pourquoi système d'information peut avoir besoin d'une solution d' archivage alors solution d'tarchivages accède tous jours donc gagne l'espace trimmons performances d'accès oui Voilà réduit cherche avoir solution stockage faible coût tout conservant don'tées cas où tout aussi faisant économies va trop dépenser juste conserver donnés potentiellement.\"}, {'summary_text': \"L'idée c'est ça donc voilà donc c'évidemment. Service stockage données coûte plus cher solutions d'archivage donc s3. Aujourd'hui va passer chapitre suivant alors si pose question pourquoi système d'information peut avoir besoin d'une solution d' archivage alors solution d'tarchivages accède tous jours donc gagne l'espace trimmons performances d'accès oui Voilà réduit cherche avoir solution stockage faible coût tout conservant don'tées cas où tout aussi faisant économies va trop dépenser juste conserver.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using maximum tokens size to increase summaries quality**"
      ],
      "metadata": {
        "id": "t23dihF_hnTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode_plus(short_aws_resume, max_length=1024, truncation=True)"
      ],
      "metadata": {
        "id": "SAj4sl_wjK5p"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPLiow9BlVM3",
        "outputId": "1d6c14a7-84f8-49c2-90fe-6bc6c0ba9f30"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [0, 2794, 897, 9650, 9374, 2520, 1076, 994, 1076, 994, 4112, 748, 1827, 784, 108, 1439, 385, 405, 475, 5655, 4112, 2084, 15367, 642, 8025, 859, 2084, 17867, 29, 6534, 4472, 952, 6248, 102, 295, 1140, 19348, 1459, 9689, 1688, 2704, 385, 108, 8007, 34276, 21193, 263, 19314, 241, 233, 9650, 1192, 9650, 2677, 108, 718, 842, 10768, 8508, 459, 385, 1253, 7427, 162, 4061, 4123, 1274, 4400, 9650, 1192, 748, 1827, 748, 1827, 1177, 8546, 1459, 329, 8477, 4532, 385, 108, 7904, 3109, 218, 438, 4112, 3137, 3320, 242, 8113, 748, 1827, 9650, 8593, 254, 2694, 1142, 7675, 295, 108, 219, 10, 1236, 2583, 354, 263, 864, 741, 5563, 859, 326, 995, 3304, 6979, 1140, 5224, 927, 326, 33514, 326, 33514, 10405, 4400, 7675, 3087, 952, 6248, 102, 13205, 1192, 10, 5326, 2084, 17867, 29, 4112, 16022, 1794, 8635, 741, 18692, 1935, 1069, 18655, 7458, 118, 1021, 3371, 4279, 9334, 218, 438, 218, 438, 10, 11591, 2126, 417, 108, 298, 3371, 15, 13205, 17663, 8477, 38328, 405, 241, 2628, 1879, 927, 1076, 994, 3391, 4112, 748, 1827, 7277, 897, 864, 9650, 2253, 12467, 385, 1253, 542, 13550, 620, 8025, 1794, 263, 385, 108, 31480, 9650, 2253, 12467, 385, 1253, 45, 241, 9437, 15, 3723, 1182, 6402, 13878, 9988, 23228, 385, 108, 4438, 2472, 385, 108, 13161, 1879, 1580, 1076, 994, 9650, 897, 2472, 385, 108, 13161, 1879, 1580, 952, 6248, 102, 6821, 5236, 2677, 463, 15, 10, 2694, 218, 282, 1140, 293, 8477, 17418, 15, 7678, 8025, 2794, 6977, 326, 1827, 7427, 1236, 5634, 4400, 218, 438, 15, 821, 16784, 263, 784, 108, 21359, 4450, 1021, 257, 2694, 2664, 13255, 1790, 2694, 4476, 385, 108, 7904, 8025, 29, 1021, 3371, 17377, 718, 5269, 15, 15367, 6588, 405, 15, 30379, 2871, 6534, 6402, 13878, 12515, 2472, 263, 388, 1580, 15087, 3304, 6534, 18363, 4748, 1029, 27564, 90, 326, 995, 1177, 23923, 927, 2084, 218, 282, 1140, 293, 8477, 9074, 1021, 3849, 9253, 4400, 4400, 326, 995, 1177, 10, 4781, 118, 1177, 856, 5655, 927, 2694, 7935, 35706, 918, 15, 13205, 6977, 37473, 10768, 642, 43897, 95, 242, 9650, 7407, 39540, 2694, 218, 282, 1140, 293, 1192, 16139, 5255, 13767, 15, 13205, 1236, 2583, 354, 979, 1807, 710, 475, 5655, 15, 3304, 17035, 12042, 263, 7427, 23118, 254, 8477, 9074, 1021, 3849, 9253, 4279, 9334, 7427, 2380, 4112, 748, 1827, 7427, 3349, 5739, 1342, 5841, 4112, 181, 9401, 6977, 3391, 15, 3304, 37530, 705, 1140, 6534, 2677, 523, 1690, 1829, 9650, 7427, 2380, 95, 242, 2084, 2843, 952, 6248, 102, 228, 5646, 263, 40614, 523, 254, 2084, 8541, 3463, 859, 4112, 2084, 856, 5655, 6379, 6285, 218, 438, 784, 108, 808, 9703, 740, 108, 990, 952, 6248, 102, 218, 438, 17377, 718, 5269, 218, 438, 740, 108, 990, 542, 544, 263, 388, 1580, 263, 218, 282, 1140, 293, 740, 108, 990, 9689, 1688, 12515, 856, 12992, 740, 108, 990, 7034, 242, 7427, 7531, 218, 282, 1140, 293, 10, 25369, 897, 25871, 1140, 21557, 740, 108, 990, 1192, 7427, 7531, 218, 282, 1140, 293, 22272, 9399, 952, 6248, 102, 1029, 27564, 859, 2704, 30379, 4400, 7675, 29, 579, 2533, 15357, 179, 5739, 9650, 385, 108, 1469, 13878, 2694, 218, 282, 1140, 293, 15087, 98, 405, 3834, 6534, 41329, 15087, 98, 405, 15087, 98, 405, 12777, 868, 22272, 9399, 2664, 8025, 29, 6664, 1140, 2253, 991, 1757, 17377, 718, 5269, 4400, 952, 6248, 102, 1029, 27564, 859, 2704, 30379, 1192, 7427, 2643, 385, 108, 13161, 1879, 1580, 218, 438, 2084, 2084, 579, 246, 952, 6248, 102, 228, 5646, 263, 23118, 254, 12515, 13295, 858, 11734, 261, 1452, 12427, 1140, 2242, 27839, 952, 6248, 102, 15, 2084, 385, 405, 2242, 27839, 6534, 385, 108, 4255, 1535, 2643, 385, 108, 13161, 1879, 1580, 7935, 21157, 991, 1757, 740, 108, 990, 6977, 740, 108, 990, 6977, 7427, 475, 5563, 12579, 228, 12782, 2694, 7531, 218, 282, 1140, 293, 218, 438, 17377, 718, 5269, 10, 25369, 897, 181, 26642, 10, 25369, 12515, 27949, 873, 12427, 1140, 9538, 10, 25369, 897, 579, 1140, 17742, 21193, 10, 25369, 7427, 4476, 218, 438, 8477, 295, 2088, 1180, 2694, 2694, 512, 7257, 1140, 11516, 16474, 7427, 512, 7257, 1140, 11516, 16474, 218, 438, 10, 25369, 579, 246, 15, 3304, 4453, 263, 388, 254, 2694, 856, 1725, 4733, 1236, 687, 2253, 108, 5269, 897, 7886, 459, 263, 195, 8470, 1535, 2084, 388, 1580, 7675, 3304, 4812, 757, 21193, 7427, 388, 1580, 7675, 3304, 856, 5236, 385, 1253, 2694, 3298, 257, 16683, 7427, 3298, 257, 16683, 740, 108, 990, 7427, 3298, 257, 16683, 740, 108, 990, 95, 242, 12515, 6010, 7425, 5150, 263, 13736, 218, 282, 1140, 293, 740, 108, 990, 7034, 242, 7427, 15367, 1741, 560, 7948, 8113, 897, 3563, 17377, 718, 5269, 2084, 766, 980, 740, 108, 990, 542, 8538, 5317, 4112, 748, 1827, 40614, 6591, 1192, 4112, 748, 1827, 10, 118, 385, 405, 740, 3398, 740, 3398, 335, 740, 108, 990, 6534, 10697, 14547, 118, 740, 108, 990, 6534, 10697, 3391, 7458, 118, 4112, 14310, 354, 354, 542, 22693, 263, 3298, 257, 3398, 1177, 13597, 740, 108, 990, 2216, 6977, 842, 257, 13767, 385, 1253, 6154, 766, 980, 2242, 27839, 6534, 6154, 3018, 28467, 786, 740, 108, 990, 6977, 2242, 27839, 6534, 6154, 29853, 842, 257, 13767, 475, 5655, 740, 108, 990, 2216, 6534, 784, 108, 1140, 22448, 459, 475, 2832, 2617, 242, 6534, 784, 108, 1140, 22448, 459, 26177, 17377, 718, 5269, 6534, 784, 108, 1140, 22448, 459, 542, 8538, 5317, 459, 740, 108, 990, 6534, 10697, 14547, 118, 3391, 7458, 118, 4112, 18363, 6619, 354, 542, 22693, 10, 3964, 879, 2242, 5408, 748, 1827, 15087, 3723, 1182, 2084, 14310, 354, 853, 17377, 718, 5269, 1855, 1043, 879, 263, 748, 1827, 784, 994, 2253, 108, 1115, 338, 8025, 29, 385, 1253, 2084, 326, 642, 7675, 13205, 326, 11798, 263, 25241, 897, 3977, 1140, 1258, 385, 108, 879, 3298, 257, 3398, 7675, 13205, 842, 24200, 241, 3137, 3320, 242, 2677, 108, 718, 3087, 3723, 1182, 2084, 25241, 2677, 108, 718, 3087, 3723, 1182, 2084, 25241, 2677, 108, 5269, 1881, 2677, 108, 718, 3723, 1182, 7675, 3087, 3723, 1182, 3977, 1140, 254, 2694, 3298, 257, 16683, 2677, 108, 4097, 438, 2694, 295, 6806, 15087, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_aws_resume_truncated = tokenizer.decode(token_ids[\"input_ids\"]) # decode\n",
        "print(short_aws_resume_truncated) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZaQC58QjXCs",
        "outputId": "ea62f813-1b5f-4a3f-f32e-c3b41ac01f14"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>de la pour merci alors alors je vous l'ai dit mais je le répète le cours à distance ça nécessite encore plus d'interactivité de votre part pour que pour qu'il se déroule dans les meilleures conditions et pour que vous vous en profitez au maximum d'accord donc je compte sur vous pour poser des questions il n'y a jamais de question bête tout est intéressant toute toute interaction et il ne ça va que aider le cours je ferme ce bazar derrière moi oui du coup donc donc aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information pourquoi dans notre architecture on peut avoir besoin d'une solution d'archivage alors pour la solution d'archivage ça serait quand on a des données auquel on accède pas tous les jours et donc on gagne de l'espace ou des trimmons des performances d'accès oui voilà on réduit on cherche à avoir une solution de stockage qui est à faible coût tout en conservant le données au cas où et et tout en aussi en faisant des économies on va pas trop dépenser juste pour conserver des données que potentiellement on va jamais son serveur mais on est contraint de les garder au cas où du coup les sides je vous les présentez je pense pas si on est arrivé à quel stade pour les sides juste le 38 ça permet de rappeler le contexte je le fais rapidement donc l'idée c'est ça donc voilà donc c'est un service de stockage de données c'est encore une fois c'est comme les bases données avec la différence c'est que les bases données souvent ça coûte plus cher et ils sont destinés pour d'avoir des données qui soit mis à jour qui soit qui soit consultable souvent très fréquemment voilà et ça coûte plus cher que les solutions d'archivage donc le le s3 ça permet de garder une bonne disponibilité par rapport ça on le dit par rapport à d'autres solutions d'archivage évidemment c'est pas c'est pas les mêmes pertes des bases données donc voilà avec la pouf avec une scalabilité flexible avec la sécurité avec les performances donc au niveau des des caractéristiques les caractéristiques donc avec s3 on est capable de stocker des fichiers jusqu'à la taille de 5 terres le stockage il est illimité les stockage il est fait dans des baguettes les baguettes c'est les baguettes c'est juste une organisation logique de nos données c'est comme les répertoires sur la machine voilà le name space c'est universel je vous rappelle que je vous ai dit cette cette information c'est à dire quoi c'est à dire si moi je choisis un nom de baguette enfin c'est unique pas seulement dans mon name space par rapport à mon user assign non c'est pas par rapport à mon alias seulement mais c'est unique à l'échelle mondiale à l'échelle AWS voilà à l'échelle universelle c'est à dire quoi si moi je faisais un nom aucun parmi vous qui peut le choisir voilà chacun de vous lorsqu'après dans le tp il va tenter de faire la création d'un baguette il va se rendre compte qu'il ne peut le faire qu'il ne peut le faire qu'à condition qu'il peut il ne peut créer des baguettes qu'avec des noms qui</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_aws_resume_truncated = short_aws_resume_truncated[3:-4]"
      ],
      "metadata": {
        "id": "5TZQPro8l-X6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(short_aws_resume_truncated) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20VqcZmXlmJO",
        "outputId": "916a3bb0-9e69-49cc-9988-227c0fad64c1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de la pour merci alors alors je vous l'ai dit mais je le répète le cours à distance ça nécessite encore plus d'interactivité de votre part pour que pour qu'il se déroule dans les meilleures conditions et pour que vous vous en profitez au maximum d'accord donc je compte sur vous pour poser des questions il n'y a jamais de question bête tout est intéressant toute toute interaction et il ne ça va que aider le cours je ferme ce bazar derrière moi oui du coup donc donc aujourd'hui on va passer au chapitre suivant alors si je vous pose la question pourquoi dans un système de d'information pourquoi dans notre architecture on peut avoir besoin d'une solution d'archivage alors pour la solution d'archivage ça serait quand on a des données auquel on accède pas tous les jours et donc on gagne de l'espace ou des trimmons des performances d'accès oui voilà on réduit on cherche à avoir une solution de stockage qui est à faible coût tout en conservant le données au cas où et et tout en aussi en faisant des économies on va pas trop dépenser juste pour conserver des données que potentiellement on va jamais son serveur mais on est contraint de les garder au cas où du coup les sides je vous les présentez je pense pas si on est arrivé à quel stade pour les sides juste le 38 ça permet de rappeler le contexte je le fais rapidement donc l'idée c'est ça donc voilà donc c'est un service de stockage de données c'est encore une fois c'est comme les bases données avec la différence c'est que les bases données souvent ça coûte plus cher et ils sont destinés pour d'avoir des données qui soit mis à jour qui soit qui soit consultable souvent très fréquemment voilà et ça coûte plus cher que les solutions d'archivage donc le le s3 ça permet de garder une bonne disponibilité par rapport ça on le dit par rapport à d'autres solutions d'archivage évidemment c'est pas c'est pas les mêmes pertes des bases données donc voilà avec la pouf avec une scalabilité flexible avec la sécurité avec les performances donc au niveau des des caractéristiques les caractéristiques donc avec s3 on est capable de stocker des fichiers jusqu'à la taille de 5 terres le stockage il est illimité les stockage il est fait dans des baguettes les baguettes c'est les baguettes c'est juste une organisation logique de nos données c'est comme les répertoires sur la machine voilà le name space c'est universel je vous rappelle que je vous ai dit cette cette information c'est à dire quoi c'est à dire si moi je choisis un nom de baguette enfin c'est unique pas seulement dans mon name space par rapport à mon user assign non c'est pas par rapport à mon alias seulement mais c'est unique à l'échelle mondiale à l'échelle AWS voilà à l'échelle universelle c'est à dire quoi si moi je faisais un nom aucun parmi vous qui peut le choisir voilà chacun de vous lorsqu'après dans le tp il va tenter de faire la création d'un baguette il va se rendre compte qu'il ne peut le faire qu'il ne peut le faire qu'à condition qu'il peut il ne peut créer des baguettes qu'avec des noms qui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarize = summarizer(short_aws_resume_truncated, max_length=1024, min_length=526, do_sample=False, num_beams=7, num_return_sequences=2)"
      ],
      "metadata": {
        "id": "2_cDxkDRhcR0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for elem in summarize:\n",
        "  pprint.pprint(elem[\"summary_text\"].replace(\"\\xa0\", \" \")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7IYwPzJfe3o",
        "outputId": "5d0df459-fd98-452c-c116-9a3febf48131"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Aujourd'hui on va passer au chapitre suivant alors si je vous pose la \"\n",
            " \"question pourquoi dans un système de d'information. On peut avoir besoin \"\n",
            " \"d'une solution d'archivage alors pour la solution d’archivages on accède pas \"\n",
            " \"tous les jours. On gagne de l'espace ou des trimmons des performances \"\n",
            " \"d'accès oui voilà on réduit on cherche à avoir une solution de stockage. On \"\n",
            " 'va pas trop dépenser juste pour conserver des données that potentiellement '\n",
            " 'on va jamais son serveur mais on est contraint de les garder au cas où du '\n",
            " 'coup les sides je vou les présentez. On est capable de stocker des fichiers '\n",
            " \"jusqu'à la taille de 5 terres le stockage il est illimité les stockage i.e. \"\n",
            " \"les baguettes c'est juste une organisation logique de nos donn Répertoires \"\n",
            " 'sur la machine. Le s3 permet de garder une bonne disponibilité par rapport '\n",
            " \"ça on le dit par rapport à d'autres solutions d' archivage évidemment. Le \"\n",
            " \"name space c'échelle l'est unique pas seulement dans mon name space par \"\n",
            " \"rapport  à l'iale à mon alias seulements mais  un nom de baguette l'is \"\n",
            " \"unique l'echelle mondé l'aussi l'an de la pour merci alors alors. C'est pour \"\n",
            " \"qu'il se déroule dans les meilleures conditions et pour que vous vous en \"\n",
            " \"profitez au maximum d'saccord donc je compte sur vous pour poser des \"\n",
            " \"questions il n'y a jamai de question bête tout est intéressant toute toute \"\n",
            " 'interaction et il ne Ýa va que aider le cours je ferme ce bazar derrière moi '\n",
            " \"oui du coup donc donc aujourhui  on va passer   suivant  à l'endroit de la \"\n",
            " \"bazar de la bazar de la Bazar de Luxembourg. Le name space c'est universel \"\n",
            " \"je vus rappelle que je  dit cette cette information c'er à dire quoi c'st à \"\n",
            " 'dire si moi je choisis un nom de baguette enfin.')\n",
            "(\"Aujourd'hui on va passer au chapitre suivant alors si je vous pose la \"\n",
            " \"question pourquoi dans un système de d'information. On peut avoir besoin \"\n",
            " \"d'une solution d'archivage alors pour la solution d’archivages on accède pas \"\n",
            " \"tous les jours. On gagne de l'espace ou des trimmons des performances \"\n",
            " \"d'accès oui voilà on réduit on cherche à avoir une solution de stockage. On \"\n",
            " 'va pas trop dépenser juste pour conserver des données that potentiellement '\n",
            " 'on va jamais son serveur mais on est contraint de les garder au cas où du '\n",
            " 'coup les sides je vou les présentez. On est capable de stocker des fichiers '\n",
            " \"jusqu'à la taille de 5 terres le stockage il est illimité les stockage i.e. \"\n",
            " \"les baguettes c'est juste une organisation logique de nos donn Répertoires \"\n",
            " 'sur la machine. Le s3 permet de garder une bonne disponibilité par rapport '\n",
            " \"ça on le dit par rapport à d'autres solutions d' archivage évidemment. Le \"\n",
            " \"name space c'échelle l'est unique pas seulement dans mon name space par \"\n",
            " \"rapport  à l'iale à mon alias seulements mais  un nom de baguette l'is \"\n",
            " \"unique l'echelle mondé l'aussi l'an de la pour merci alors alors. C'est pour \"\n",
            " \"qu'il se déroule dans les meilleures conditions et pour que vous vous en \"\n",
            " \"profitez au maximum d'saccord donc je compte sur vous pour poser des \"\n",
            " \"questions il n'y a jamai de question bête tout est intéressant toute toute \"\n",
            " 'interaction et il ne Ýa va que aider le cours je ferme ce bazar derrière moi '\n",
            " \"oui du coup donc donc aujourhui  on va passer   suivant  à l'endroit de la \"\n",
            " \"bazar de la bazar de la Bazar de Luxembourg. Le name space c'est universel \"\n",
            " \"je vus rappelle que je  dit cette cette information c'er à dire quoi c'st à \"\n",
            " \"dire si moi je choisis un nom de baguette enfin c'edite.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using GPT2 to generate sentence from keywords**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mpUxEKNbqT58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_sentence = \"last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II\""
      ],
      "metadata": {
        "id": "A0DBQDPLqTOz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "nNcsov3ynR0m"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(keywords_sentence, max_length=100, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agxFx_UVqqY1",
        "outputId": "a2e66c3c-a8ec-42b2-fed7-f16e3b084589"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II: Part I: The Big Book of Life Daniel Radcliffe The Big Book of Life James Bond John Lithgow the man who became the highest selling actor in cinema'},\n",
              " {'generated_text': \"last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II 'The House You Hate is a place for real people to live and work in the Australian community. And it's not even in the 'The House You Hate\"},\n",
              " {'generated_text': \"last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II, Part III, A/D Interview with Jonny Mitchell Jonny Mitchell's new book A Dangerous Sex Book with Jonny Mitchell, with Gary Vinson Jon\"},\n",
              " {'generated_text': 'last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II Australian film Daniel Radcliffe the horror film a massive sports car collection includes: a £6,000 four wheel motor and a £16,000 petrol one\\n'},\n",
              " {'generated_text': 'last month celebrity parties Harry Potter star Daniel Radcliffe Harry Potter fast cars party Potter author Rudyard Kipling Part II Rudyard Kipling the UK box office chart kid star DVDs Reuters gossip columnists Australian release fair game UK an Australian film Daniel Radcliffe the horror film a massive sports car collection wraps Potters latest » Hostel: Part II Hostel: Part III London nightlife of the year A party hosted by actor Bill Nye, a former British spy, Bill Murray, a former US ambassador'}]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}